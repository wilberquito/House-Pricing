{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model and deploy it into WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/repos/lightning/HousePricing'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path.cwd()\n",
    "os.chdir(p.parent)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['WANDB_API_KEY', 'WANDB_NAME'])\n",
      "odict_keys(['WANDB_NOTEBOOK_NAME', 'ITERATIONS', 'MAX_EPOCHS', 'PATIENCE', 'BATCH_SIZE', 'LEARNING_RATE', 'VALIDATION_SIZE'])\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "import os\n",
    "\n",
    "envs = [\"secret.env\", \"fit.env\"]\n",
    "\n",
    "for fenv in envs:\n",
    "    file = os.path.join(\"env\", fenv)\n",
    "    config = dotenv_values(file)  # load sensitive variables\n",
    "    print(config.keys())\n",
    "    for c, v in config.items():\n",
    "        os.environ[c] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwilber-quito\u001b[0m (\u001b[33mdeepsat\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "wandb_key = os.environ[\"WANDB_API_KEY\"]\n",
    "wandb.login(key=wandb_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "max(1, os.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Skipping downloading data. Data is already downloaded\n",
      "[INFO]: Reloading set up data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/repos/lightning/HousePricing/wandb/run-20240422_131440-mrsat7bm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deepsat/House%20Pricing/runs/mrsat7bm' target=\"_blank\">Mr._ZI1EUZUC</a></strong> to <a href='https://wandb.ai/deepsat/House%20Pricing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deepsat/House%20Pricing' target=\"_blank\">https://wandb.ai/deepsat/House%20Pricing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deepsat/House%20Pricing/runs/mrsat7bm' target=\"_blank\">https://wandb.ai/deepsat/House%20Pricing/runs/mrsat7bm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Fit config: {'accelerator': 'gpu', 'max_epochs': 1000, 'patience': 100, 'lr': 0.001, 'batch_size': 256, 'in_features': 244, 'validation_size': 0.1}\n",
      "[INFO]: Input size: 244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | net  | Sequential | 215 K \n",
      "------------------------------------\n",
      "215 K     Trainable params\n",
      "0         Non-trainable params\n",
      "215 K     Total params\n",
      "0.864     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Skipping downloading data. Data is already downloaded\n",
      "[INFO]: Reloading set up data...\n",
      "[INFO]: Setting up fit dataset/s\n",
      "[INFO]: Train dataset size: 1166\n",
      "[INFO]: Validation dataset size: 129\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s][INFO]: Validation dataloader size: 2\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Train dataloader size: 5\n",
      "Training: |          | 0/? [00:00<?, ?it/s][INFO]: Logger save model dir: ./lightning_logs/mrsat7bm\n",
      "Epoch 999: 100%|██████████| 5/5 [00:00<00:00, 37.91it/s, v_num=t7bm]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 5/5 [00:00<00:00, 36.89it/s, v_num=t7bm]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>999</td></tr><tr><td>train_loss</td><td>177689472.0</td></tr><tr><td>trainer/global_step</td><td>4999</td></tr><tr><td>val_loss</td><td>387328832.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Mr._ZI1EUZUC</strong> at: <a href='https://wandb.ai/deepsat/House%20Pricing/runs/mrsat7bm' target=\"_blank\">https://wandb.ai/deepsat/House%20Pricing/runs/mrsat7bm</a><br/> View project at: <a href='https://wandb.ai/deepsat/House%20Pricing' target=\"_blank\">https://wandb.ai/deepsat/House%20Pricing</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240422_131440-mrsat7bm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/repos/lightning/HousePricing/wandb/run-20240422_131727-3478itnz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deepsat/House%20Pricing/runs/3478itnz' target=\"_blank\">Gerald_4LIP1BLK</a></strong> to <a href='https://wandb.ai/deepsat/House%20Pricing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deepsat/House%20Pricing' target=\"_blank\">https://wandb.ai/deepsat/House%20Pricing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deepsat/House%20Pricing/runs/3478itnz' target=\"_blank\">https://wandb.ai/deepsat/House%20Pricing/runs/3478itnz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | net  | Sequential | 215 K \n",
      "------------------------------------\n",
      "215 K     Trainable params\n",
      "0         Non-trainable params\n",
      "215 K     Total params\n",
      "0.864     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Fit config: {'accelerator': 'gpu', 'max_epochs': 1000, 'patience': 100, 'lr': 0.001, 'batch_size': 256, 'in_features': 244, 'validation_size': 0.1}\n",
      "[INFO]: Input size: 244\n",
      "[INFO]: Skipping downloading data. Data is already downloaded\n",
      "[INFO]: Reloading set up data...\n",
      "[INFO]: Setting up fit dataset/s\n",
      "[INFO]: Train dataset size: 1166\n",
      "[INFO]: Validation dataset size: 129\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s][INFO]: Validation dataloader size: 2\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Train dataloader size: 5\n",
      "Training: |          | 0/? [00:00<?, ?it/s][INFO]: Logger save model dir: ./lightning_logs/3478itnz\n",
      "Epoch 931: 100%|██████████| 5/5 [00:00<00:00, 33.51it/s, v_num=itnz]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>931</td></tr><tr><td>train_loss</td><td>105884808.0</td></tr><tr><td>trainer/global_step</td><td>4659</td></tr><tr><td>val_loss</td><td>393353344.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Gerald_4LIP1BLK</strong> at: <a href='https://wandb.ai/deepsat/House%20Pricing/runs/3478itnz' target=\"_blank\">https://wandb.ai/deepsat/House%20Pricing/runs/3478itnz</a><br/> View project at: <a href='https://wandb.ai/deepsat/House%20Pricing' target=\"_blank\">https://wandb.ai/deepsat/House%20Pricing</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240422_131727-3478itnz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/repos/lightning/HousePricing/wandb/run-20240422_132001-27yrkxwa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deepsat/House%20Pricing/runs/27yrkxwa' target=\"_blank\">Stacey_Y41HGQZW</a></strong> to <a href='https://wandb.ai/deepsat/House%20Pricing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deepsat/House%20Pricing' target=\"_blank\">https://wandb.ai/deepsat/House%20Pricing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deepsat/House%20Pricing/runs/27yrkxwa' target=\"_blank\">https://wandb.ai/deepsat/House%20Pricing/runs/27yrkxwa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | net  | Sequential | 215 K \n",
      "------------------------------------\n",
      "215 K     Trainable params\n",
      "0         Non-trainable params\n",
      "215 K     Total params\n",
      "0.864     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Fit config: {'accelerator': 'gpu', 'max_epochs': 1000, 'patience': 100, 'lr': 0.001, 'batch_size': 256, 'in_features': 244, 'validation_size': 0.1}\n",
      "[INFO]: Input size: 244\n",
      "[INFO]: Skipping downloading data. Data is already downloaded\n",
      "[INFO]: Reloading set up data...\n",
      "[INFO]: Setting up fit dataset/s\n",
      "[INFO]: Train dataset size: 1166\n",
      "[INFO]: Validation dataset size: 129\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s][INFO]: Validation dataloader size: 2\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/root/miniconda3/envs/pricehousing/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Train dataloader size: 5\n",
      "Training: |          | 0/? [00:00<?, ?it/s][INFO]: Logger save model dir: ./lightning_logs/27yrkxwa\n",
      "Epoch 918: 100%|██████████| 5/5 [00:00<00:00, 34.52it/s, v_num=kxwa]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>918</td></tr><tr><td>train_loss</td><td>109455904.0</td></tr><tr><td>trainer/global_step</td><td>4594</td></tr><tr><td>val_loss</td><td>409431008.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Stacey_Y41HGQZW</strong> at: <a href='https://wandb.ai/deepsat/House%20Pricing/runs/27yrkxwa' target=\"_blank\">https://wandb.ai/deepsat/House%20Pricing/runs/27yrkxwa</a><br/> View project at: <a href='https://wandb.ai/deepsat/House%20Pricing' target=\"_blank\">https://wandb.ai/deepsat/House%20Pricing</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240422_132001-27yrkxwa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "from lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from src.model import NeuralNetwork\n",
    "from src.data import HousePricingDataModule\n",
    "from src.utils.utility import fake_name\n",
    "\n",
    "project_name = os.environ[\"WANDB_NAME\"]\n",
    "iters = int(os.environ[\"ITERATIONS\"])\n",
    "\n",
    "patience = int(os.environ[\"PATIENCE\"])\n",
    "max_epochs = int(os.environ[\"MAX_EPOCHS\"])\n",
    "batch_size = int(os.environ[\"BATCH_SIZE\"])\n",
    "learning_rate = float(os.environ[\"LEARNING_RATE\"])\n",
    "validation_size = float(os.environ[\"VALIDATION_SIZE\"])\n",
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Preparing data to be used\n",
    "data_module = HousePricingDataModule(\n",
    "    batch_size=batch_size,\n",
    "    validation_size=validation_size,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "\n",
    "in_features = data_module.data_features()\n",
    "\n",
    "# Setting up the training configuration\n",
    "config = {\n",
    "    \"accelerator\": accelerator,\n",
    "    \"max_epochs\": max_epochs,\n",
    "    \"patience\": patience,\n",
    "    \"lr\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"in_features\": in_features,\n",
    "    \"validation_size\": validation_size,\n",
    "}\n",
    "\n",
    "artifacts = list()\n",
    "\n",
    "for i in range(iters):\n",
    "\n",
    "    run_name = fake_name()\n",
    "\n",
    "    wandb.init(\n",
    "        job_type=\"train\",\n",
    "        name=run_name,\n",
    "        project=project_name,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    print(f\"[INFO]: Fit config: {config}\")\n",
    "\n",
    "    # Defining the model to be training\n",
    "    model = NeuralNetwork(input_size=wandb.config[\"in_features\"], lr=wandb.config[\"lr\"])\n",
    "\n",
    "    # Defining the logger instance the lighning will use as default logging\n",
    "    logger = WandbLogger()\n",
    "\n",
    "    # Define how the model registry work\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        every_n_epochs=1,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=2,\n",
    "        filename=\"house_pricing-{epoch:02d}-{val_loss:.2f}\",\n",
    "    )\n",
    "\n",
    "    # Defining early stop configuration\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=False,\n",
    "        patience=wandb.config[\"patience\"],\n",
    "    )\n",
    "\n",
    "    # Defines the training instance\n",
    "    trainer = Trainer(\n",
    "        accelerator=wandb.config[\"accelerator\"],\n",
    "        max_epochs=wandb.config[\"max_epochs\"],\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    artifacts_item = {\n",
    "        \"run_name\": run_name,\n",
    "        \"best_model\": checkpoint_callback.best_model_path,\n",
    "        \"fit_config\": config,\n",
    "    }\n",
    "    artifacts.append(artifacts_item)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = list()\n",
    "for data in artifacts:\n",
    "    run_name = data[\"run_name\"]\n",
    "    best_model = data[\"best_model\"]\n",
    "    fit_config = data[\"fit_config\"]\n",
    "    fit_config = {f\"fit_{key}\": value for key, value in fit_config.items()}\n",
    "    tmp.append({\"run_name\": run_name, \"best_model\": best_model, **fit_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>best_model</th>\n",
       "      <th>fit_accelerator</th>\n",
       "      <th>fit_max_epochs</th>\n",
       "      <th>fit_patience</th>\n",
       "      <th>fit_lr</th>\n",
       "      <th>fit_batch_size</th>\n",
       "      <th>fit_in_features</th>\n",
       "      <th>fit_validation_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr._ZI1EUZUC</td>\n",
       "      <td>./lightning_logs/mrsat7bm/checkpoints/house_pr...</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>256</td>\n",
       "      <td>244</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gerald_4LIP1BLK</td>\n",
       "      <td>./lightning_logs/3478itnz/checkpoints/house_pr...</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>256</td>\n",
       "      <td>244</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stacey_Y41HGQZW</td>\n",
       "      <td>./lightning_logs/27yrkxwa/checkpoints/house_pr...</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>256</td>\n",
       "      <td>244</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          run_name                                         best_model  \\\n",
       "0     Mr._ZI1EUZUC  ./lightning_logs/mrsat7bm/checkpoints/house_pr...   \n",
       "1  Gerald_4LIP1BLK  ./lightning_logs/3478itnz/checkpoints/house_pr...   \n",
       "2  Stacey_Y41HGQZW  ./lightning_logs/27yrkxwa/checkpoints/house_pr...   \n",
       "\n",
       "  fit_accelerator  fit_max_epochs  fit_patience  fit_lr  fit_batch_size  \\\n",
       "0             gpu            1000           100   0.001             256   \n",
       "1             gpu            1000           100   0.001             256   \n",
       "2             gpu            1000           100   0.001             256   \n",
       "\n",
       "   fit_in_features  fit_validation_size  \n",
       "0              244                  0.1  \n",
       "1              244                  0.1  \n",
       "2              244                  0.1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(tmp).to_csv(\"artifacts.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pricehousing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
